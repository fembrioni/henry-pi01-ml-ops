{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el notebook donde se define y procesa el modelo de recomendación, sobre la base del uso de la distancia coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import etl_flow as etlflow\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DIRECTORIO_RAIZ=/com.docker.devenvironments.code\n"
     ]
    }
   ],
   "source": [
    "# Variables de ambiente (Para procesamiento en ambiente de desarrollo)\n",
    "%env DIRECTORIO_RAIZ=/com.docker.devenvironments.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupero el movies dataset preprocesado\n",
    "m_df = etlflow.obtener_df_preprocesado('m_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas no utilizadas. Conservar solo id y overview\n",
    "m_df = m_df[['id', 'overview']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, las funciones que permiten el preprocesamiento del modelo de recomendacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina signos de puntuacion de una lista tokenizada\n",
    "def limpia_signos_de_puntuacion(lista_tokens: list):\n",
    "    token_rta = []\n",
    "    for palabra in lista_tokens:\n",
    "        for letra in palabra:\n",
    "            if letra in string.punctuation:\n",
    "                palabra=palabra.replace(letra,\"\")\n",
    "        token_rta.append(palabra)\n",
    "    return token_rta\n",
    "\n",
    "# Elimina numeros\n",
    "def limpia_numeros(lista_tokens: list):\n",
    "    token_rta = []\n",
    "    for palabra in lista_tokens:\n",
    "        for letra in palabra:\n",
    "            if letra in string.digits:\n",
    "                palabra=palabra.replace(letra,\"\")\n",
    "        token_rta.append(palabra)\n",
    "    return token_rta\n",
    "\n",
    "# Elimina tokens vacios\n",
    "def elimina_tokens_vacios(lista_tokens: list):\n",
    "    token_rta = []\n",
    "    for palabra in lista_tokens:\n",
    "        if palabra != \"\":\n",
    "            token_rta.append(palabra)\n",
    "    return token_rta\n",
    "\n",
    "# Pasa los tokens a minusculas\n",
    "def pasa_tokens_a_minusculas(lista_tokens: list):\n",
    "    token_rta = []\n",
    "    for palabra in lista_tokens:\n",
    "        token_rta.append(palabra.lower())\n",
    "    return token_rta\n",
    "\n",
    "# Elimina tokens cortos\n",
    "def elimina_tokens_cortos(lista_tokens: list):\n",
    "    token_rta = []\n",
    "    for palabra in lista_tokens:\n",
    "        if len(palabra)>=3:\n",
    "            token_rta.append(palabra)\n",
    "    return token_rta\n",
    "\n",
    "# Elimina stop words\n",
    "def elimina_stop_words(lista_tokens: list):\n",
    "    a=set(stopwords.words('english'))\n",
    "    token_rta = [palabra for palabra in lista_tokens if palabra not in a]\n",
    "    return token_rta\n",
    "\n",
    "# Tokeniza el texto y limpia la lista tokenizada\n",
    "def tokenize_and_clean(texto: str):\n",
    "    token = word_tokenize(texto) # Tokenizo\n",
    "    token = limpia_signos_de_puntuacion(token) # Limpio signos de puntuacion\n",
    "    token = limpia_numeros(token) # Limpio números\n",
    "    token = elimina_tokens_vacios(token) # Elimino tokens vacios\n",
    "    token = pasa_tokens_a_minusculas(token) # Paso los tokens a minusculas\n",
    "    token = elimina_tokens_cortos(token) # Elimino tokens cortos\n",
    "    token = elimina_stop_words(token) # Elimino stop words del Inglés, de la lista de tokens\n",
    "    return token\n",
    "\n",
    "# Tokeniza, limpia y vuelve a armar en forma de string\n",
    "def tokenizar_limpiar_y_obtener_string(texto: str):\n",
    "    token_limpio = tokenize_and_clean(texto)\n",
    "    t_str = ' '.join(token_limpio)\n",
    "    return t_str\n",
    "\n",
    "# Serializador / Deserializador de objetos\n",
    "serializados_d = {'mtx_tfidf' : ['src/preproc/mtx_tfidf.ser', '\\t']}\n",
    "\n",
    "def serializar(nombre_objeto: str, objeto: any):\n",
    "       \n",
    "       if nombre_objeto in serializados_d.keys():\n",
    "\n",
    "              # Obtengo el directorio raiz desde la variable de entorno DIRECTORIO_RAIZ\n",
    "              dir_raiz = os.getenv(\"DIRECTORIO_RAIZ\")\n",
    "\n",
    "              # Escribo el objeto al archivo correspondiente\n",
    "              path_archivo = os.path.join(dir_raiz, serializados_d[nombre_objeto][0])\n",
    "              with open(path_archivo, \"wb\") as archivo:\n",
    "                  pickle.dump(objeto, archivo)\n",
    "\n",
    "       else:\n",
    "\n",
    "              # Error. El serializable no esta en la lista de serializables\n",
    "              print('Error: El serializable no esta en la lista de serializables preprocesados')\n",
    "\n",
    "def deserealizar(nombre_objeto: str):\n",
    "\n",
    "       if nombre_objeto in serializados_d.keys():\n",
    "\n",
    "              # Obtengo el directorio raiz desde la variable de entorno DIRECTORIO_RAIZ\n",
    "              dir_raiz = os.getenv(\"DIRECTORIO_RAIZ\")\n",
    "\n",
    "              # Obtengo el objeto desde el archivo correspondiente\n",
    "              objeto = None\n",
    "              path_archivo = os.path.join(dir_raiz, serializados_d[nombre_objeto][0])\n",
    "              with open(path_archivo, \"rb\") as archivo:\n",
    "                   objeto = pickle.load(archivo)\n",
    "\n",
    "              return objeto\n",
    "       \n",
    "       else:\n",
    "\n",
    "              # Error. El objeto no ha sido preprocesado\n",
    "              print('Error: El objeto no esta en la lista de serializables preprocesados')\n",
    "              return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino registros con valores nulos en la columna de overview\n",
    "m_df.dropna(subset=['overview'], inplace=True)\n",
    "m_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizo, limpio y rearmo los overviews\n",
    "m_df['cleansed_overview'] = m_df['overview'].apply(tokenizar_limpiar_y_obtener_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancio un count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo la matriz Tf-Idf\n",
    "documentos = m_df['cleansed_overview']\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializar('mtx_tfidf', tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix2 = deserealizar('mtx_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b\n"
     ]
    }
   ],
   "source": [
    "tupla = ('a', 'b')\n",
    "a = tupla[0]\n",
    "b = tupla[1]\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19588/370242538.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;31m# Recupero el vectorizador entrenado y la matriz TFIDF para realizar las recomendaciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtupla_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserealizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mtx_tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtupla_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Instancio el vectorizador entrenado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmtx_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtupla_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Instancio la matriz preprocesada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \"\"\"\n\u001b[1;32m   2155\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Recupero el vectorizador entrenado y la matriz TFIDF para realizar las recomendaciones\n",
    "tupla_tfidf = deserealizar('mtx_tfidf')\n",
    "tfidf_vectorizer = tupla_tfidf[0] # Instancio el vectorizador entrenado\n",
    "mtx_tfidf = tupla_tfidf[1] # Instancio la matriz preprocesada\n",
    "\n",
    "# Elijo el titulo de este film como query document\n",
    "registro = m_df.head(1)\n",
    "query_document = registro['overview']\n",
    "\n",
    "# Sobre la base de la matriz TFIDF busco los indices de documentos similares\n",
    "query_tfidf = tfidf_vectorizer.transform([query_document])\n",
    "similarity_scores = cosine_similarity(query_tfidf, mtx_tfidf)\n",
    "similarity_scores = similarity_scores.flatten()  # Convierto a un 1D array\n",
    "related_documents_indices = similarity_scores.argsort()[::-1]  # Ordeno los indices por orden descendente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, numpy.int64 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m top_n \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     13\u001b[0m top_documents \u001b[39m=\u001b[39m [documentos[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m related_documents_indices[:top_n]]\n\u001b[0;32m---> 15\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(related_documents_indices)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(s)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(top_documents)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, numpy.int64 found"
     ]
    }
   ],
   "source": [
    "# Pruebo con un caso\n",
    "# Step 5: Choose a query document\n",
    "query_document = \"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\"\n",
    "\n",
    "# Step 6: Find similar documents\n",
    "query_tfidf = tfidf_vectorizer.transform([query_document])\n",
    "similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix2)\n",
    "similarity_scores = similarity_scores.flatten()  # Convert to 1D array\n",
    "related_documents_indices = similarity_scores.argsort()[::-1]  # Sort indices in descending order\n",
    "\n",
    "# Top N similar documents\n",
    "top_n = 5\n",
    "top_documents = [documentos[index] for index in related_documents_indices[:top_n]]\n",
    "\n",
    "s = ' '.join(related_documents_indices)\n",
    "print(s)\n",
    "print(top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaron</th>\n",
       "      <th>aabgamma</th>\n",
       "      <th>aachan</th>\n",
       "      <th>aachi</th>\n",
       "      <th>aackerlund</th>\n",
       "      <th>aadhavan</th>\n",
       "      <th>aadland</th>\n",
       "      <th>aaicha</th>\n",
       "      <th>aakash</th>\n",
       "      <th>...</th>\n",
       "      <th>ரமண</th>\n",
       "      <th>శమ</th>\n",
       "      <th>แพร</th>\n",
       "      <th>たけみかずち</th>\n",
       "      <th>ようなもの</th>\n",
       "      <th>患者さんとその世界</th>\n",
       "      <th>주식회사</th>\n",
       "      <th>첫사랑</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44417</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44418</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44419</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44420</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44421</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44422 rows × 83703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aaa  aaaron  aabgamma  aachan  aachi  aackerlund  aadhavan  aadland  \\\n",
       "0      0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "1      0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "2      0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "3      0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "4      0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "...    ...     ...       ...     ...    ...         ...       ...      ...   \n",
       "44417  0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "44418  0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "44419  0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "44420  0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "44421  0.0     0.0       0.0     0.0    0.0         0.0       0.0      0.0   \n",
       "\n",
       "       aaicha  aakash  ...  ரமண   శమ  แพร  たけみかずち  ようなもの  患者さんとその世界  주식회사  \\\n",
       "0         0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "1         0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "2         0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "3         0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "4         0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "...       ...     ...  ...  ...  ...  ...     ...    ...        ...   ...   \n",
       "44417     0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "44418     0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "44419     0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "44420     0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "44421     0.0     0.0  ...  0.0  0.0  0.0     0.0    0.0        0.0   0.0   \n",
       "\n",
       "       첫사랑  ﬁrst  ﬁve  \n",
       "0      0.0   0.0  0.0  \n",
       "1      0.0   0.0  0.0  \n",
       "2      0.0   0.0  0.0  \n",
       "3      0.0   0.0  0.0  \n",
       "4      0.0   0.0  0.0  \n",
       "...    ...   ...  ...  \n",
       "44417  0.0   0.0  0.0  \n",
       "44418  0.0   0.0  0.0  \n",
       "44419  0.0   0.0  0.0  \n",
       "44420  0.0   0.0  0.0  \n",
       "44421  0.0   0.0  0.0  \n",
       "\n",
       "[44422 rows x 83703 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veo la matriz en formato pandas (BORRAR esta celda luego)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(tfidf_matrix.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overview</th>\n",
       "      <th>cleansed_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>led woody andy toys live happily room andy bir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>siblings judy peter discover enchanted board g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>family wedding reignites ancient feud nextdoor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>cheated mistreated stepped women holding breat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>george banks recovered daughter wedding receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44417</th>\n",
       "      <td>30840</td>\n",
       "      <td>Yet another version of the classic epic, with ...</td>\n",
       "      <td>yet another version classic epic enough variat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44418</th>\n",
       "      <td>111109</td>\n",
       "      <td>An artist struggles to finish his work while a...</td>\n",
       "      <td>artist struggles finish work storyline cult pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44419</th>\n",
       "      <td>67758</td>\n",
       "      <td>When one of her hits goes wrong, a professiona...</td>\n",
       "      <td>one hits goes wrong professional assassin ends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44420</th>\n",
       "      <td>227506</td>\n",
       "      <td>In a small town live two brothers, one a minis...</td>\n",
       "      <td>small town live two brothers one minister one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44421</th>\n",
       "      <td>461257</td>\n",
       "      <td>50 years after decriminalisation of homosexual...</td>\n",
       "      <td>years decriminalisation homosexuality director...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44422 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           overview  \\\n",
       "0         862  Led by Woody, Andy's toys live happily in his ...   \n",
       "1        8844  When siblings Judy and Peter discover an encha...   \n",
       "2       15602  A family wedding reignites the ancient feud be...   \n",
       "3       31357  Cheated on, mistreated and stepped on, the wom...   \n",
       "4       11862  Just when George Banks has recovered from his ...   \n",
       "...       ...                                                ...   \n",
       "44417   30840  Yet another version of the classic epic, with ...   \n",
       "44418  111109  An artist struggles to finish his work while a...   \n",
       "44419   67758  When one of her hits goes wrong, a professiona...   \n",
       "44420  227506  In a small town live two brothers, one a minis...   \n",
       "44421  461257  50 years after decriminalisation of homosexual...   \n",
       "\n",
       "                                       cleansed_overview  \n",
       "0      led woody andy toys live happily room andy bir...  \n",
       "1      siblings judy peter discover enchanted board g...  \n",
       "2      family wedding reignites ancient feud nextdoor...  \n",
       "3      cheated mistreated stepped women holding breat...  \n",
       "4      george banks recovered daughter wedding receiv...  \n",
       "...                                                  ...  \n",
       "44417  yet another version classic epic enough variat...  \n",
       "44418  artist struggles finish work storyline cult pl...  \n",
       "44419  one hits goes wrong professional assassin ends...  \n",
       "44420  small town live two brothers one minister one ...  \n",
       "44421  years decriminalisation homosexuality director...  \n",
       "\n",
       "[44422 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.loc[0, 'overview']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
